{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: weblinx in ./venv/lib/python3.12/site-packages (0.3.2)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from weblinx) (4.67.0)\n",
      "Requirement already satisfied: huggingface_hub[cli] in ./venv/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (2024.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (6.0.2)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (4.12.2)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in ./venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in ./venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in ./venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.48)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2024.8.30)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.12/site-packages (1.54.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.12/site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: datasets in ./venv/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.12/site-packages (from datasets) (2.1.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./venv/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in ./venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in ./venv/lib/python3.12/site-packages (from datasets) (4.67.0)\n",
      "Requirement already satisfied: xxhash in ./venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./venv/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in ./venv/lib/python3.12/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.17.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install weblinx\n",
    "!pip install huggingface_hub[cli]\n",
    "!pip install openai\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weblinx data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"splits.json\") as f:\n",
    "    splits= json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from datasets import load_dataset\n",
    "\n",
    "#demo_names = ['saabwsg', 'ygprzve', 'iqaazif']\n",
    "demo_names = splits['train'] #'test_vis', 'test_geo', 'test_cat', 'test_web', 'iid_all', 'train', 'valid', 'test_iid'\n",
    "patterns = [f\"demonstrations/{name}/*.json*\" for name in demo_names]\n",
    "#patterns = [\"*.json*\"]  \n",
    "snapshot_download(\n",
    "    \"McGill-NLP/WebLINX-full\",repo_type=\"dataset\", local_dir=\"./wl_data\", allow_patterns=patterns, etag_timeout =400\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def load_json_no_cache(basedir, name):\n",
    "    if not os.path.exists(f\"{basedir}/{name}.json\"):\n",
    "        return None\n",
    "    try:\n",
    "        with open(f\"{basedir}/{name}.json\", \"r\") as f:\n",
    "            j = json.load(f)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return j\n",
    "\n",
    "\n",
    "def load_recording(basedir):\n",
    "    # Before loading replay, we need a dropdown that allows us to select replay.json or replay_orig.json\n",
    "    # Find all files in basedir starting with \"replay\" and ending with \".json\"\n",
    "    replay_files = sorted(\n",
    "        [\n",
    "            f\n",
    "            for f in os.listdir(basedir)\n",
    "            if f.startswith(\"replay\") and f.endswith(\".json\")\n",
    "        ]\n",
    "    )\n",
    "    replay_file = 'replay.json'\n",
    "    replay_file = replay_file.replace(\".json\", \"\")\n",
    "\n",
    "    metadata = load_json_no_cache(basedir, \"metadata\")\n",
    "\n",
    "\n",
    "    # Read in the JSON data\n",
    "    replay_dict = load_json_no_cache(basedir, replay_file)\n",
    "\n",
    "\n",
    "    data = replay_dict[\"data\"]\n",
    "    return data,metadata\n",
    "\n",
    "def format_chat_message(d):\n",
    "    if d[\"speaker\"] == \"instructor\":\n",
    "        return \"ðŸ§‘ \" + d[\"utterance\"]\n",
    "    else:\n",
    "        return \"ðŸ¤– \" + d[\"utterance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT API utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "\n",
    "def create_prompt(mainPrompt,mainResponse,prompt_dir):\n",
    "    onlyfiles = [f for f in listdir(prompt_dir) if isfile(join(prompt_dir, f))]\n",
    "    len_mes= int((len(onlyfiles)-1)/2)\n",
    "    messages=[{\"role\": \"user\", \"content\": mainPrompt}, {\"role\": \"assistant\", \"content\": mainResponse}]\n",
    "    for i in range(len_mes):\n",
    "        question = open(f'{prompt_dir}/question{i}.txt', \"r\").read()    \n",
    "        answer = open(f'{prompt_dir}/answer{i}.txt', \"r\").read()\n",
    "        messages.append({\"role\": \"user\", \"content\": question})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "    return messages\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class ParseError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def extract_html_tags(text, keys):\n",
    "    \"\"\"Extract the content within HTML tags for a list of keys.\n",
    "\n",
    "    All text and keys will be converted to lowercase before matching.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string containing the HTML tags.\n",
    "        keys (list[str]): The HTML tags to extract the content from.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each key to a list of subset in `text` that match the key.\n",
    "    \"\"\"\n",
    "    content_dict = {}\n",
    "    # text = text.lower()\n",
    "    # keys = set([k.lower() for k in keys])\n",
    "    for key in keys:\n",
    "        pattern = f\"<{key}>(.*?)</{key}>\"\n",
    "        matches = re.findall(pattern, text, re.DOTALL)\n",
    "        if matches:\n",
    "            content_dict[key] = [match.strip() for match in matches]\n",
    "    return content_dict\n",
    "\n",
    "def parse_html_tags(text, keys=(), optional_keys=(), merge_multiple=False):\n",
    "    \"\"\"Satisfy the parse api, extracts 1 match per key and validates that all keys are present\n",
    "\n",
    "    Args:\n",
    "        text (str): The input string containing the HTML tags.\n",
    "        keys (list[str]): The HTML tags to extract the content from.\n",
    "        optional_keys (list[str]): The HTML tags to extract the content from, but are optional.\n",
    "        merge_multiple (bool): Whether to merge multiple instances of the same key.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary mapping each key to a subset of `text` that match the key.\n",
    "        bool: Whether the parsing was successful.\n",
    "        str: A message to be displayed to the agent if the parsing was not successful.\n",
    "\n",
    "    \"\"\"\n",
    "    all_keys = tuple(keys) + tuple(optional_keys)\n",
    "    content_dict = extract_html_tags(text, all_keys)\n",
    "    retry_messages = []\n",
    "\n",
    "    for key in all_keys:\n",
    "        if not key in content_dict:\n",
    "            if not key in optional_keys:\n",
    "                retry_messages.append(f\"Missing the key <{key}> in the answer.\")\n",
    "        else:\n",
    "            val = content_dict[key]\n",
    "            content_dict[key] = val[0]\n",
    "            if len(val) > 1:\n",
    "                if not merge_multiple:\n",
    "                    retry_messages.append(\n",
    "                        f\"Found multiple instances of the key {key}. You should have only one of them.\"\n",
    "                    )\n",
    "                else:\n",
    "                    # merge the multiple instances\n",
    "                    content_dict[key] = \"\\n\".join(val)\n",
    "\n",
    "    valid = len(retry_messages) == 0\n",
    "    retry_message = \"\\n\".join(retry_messages)\n",
    "    return content_dict, valid, retry_message\n",
    "\n",
    "def parse_html_tags_raise(text, keys=(), optional_keys=(), merge_multiple=False):\n",
    "    \"\"\"A version of parse_html_tags that raises an exception if the parsing is not successful.\"\"\"\n",
    "    content_dict, valid, retry_message = parse_html_tags(\n",
    "        text, keys, optional_keys, merge_multiple=merge_multiple\n",
    "    )\n",
    "    if not valid:\n",
    "        raise ParseError(retry_message)\n",
    "    return content_dict\n",
    "\n",
    "def adapt_text(text, n_retry):\n",
    "  client = OpenAI(api_key=os.environ.get(\"OPENAI\"))\n",
    "  tries= 0\n",
    "  while tries < n_retry:\n",
    "    try:\n",
    "      prompt_dir= 'prompts/general'\n",
    "      mainPrompt= open(f'{prompt_dir}/mainPrompt.txt').read()\n",
    "      mainResponse= open(f'{prompt_dir}/mainResponse.txt').read()\n",
    "      messages = create_prompt(mainPrompt,mainResponse,prompt_dir)\n",
    "      messages.append({\"role\": \"user\", \"content\": text})\n",
    "      response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo-0125\",\n",
    "                    temperature=0.4,\n",
    "                    messages = messages\n",
    "                )\n",
    "      reply_content = response.choices[0].message.content\n",
    "      ans_dict = parse_html_tags_raise(reply_content,keys=('goal','steps') )\n",
    "      print(ans_dict)\n",
    "      return ans_dict\n",
    "    except ParseError as parsing_error:\n",
    "            tries += 1\n",
    "\n",
    "    raise ParseError(f\"Could not parse a valid value after {n_retry} retries.\")\n",
    "        \n",
    "\n",
    "\n",
    "  return reply_content "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal and Step abstraction from Weblinx\n",
    "\n",
    "The following methods abstract demonstrations from Weblinx related to a certain URL and with GPT's API extract a general goal and steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_names = [f for f in os.listdir('wl_data/demonstrations') ] #demo names available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rqrrlhc\n",
      "iuealcb\n",
      "nfkvpow\n",
      "yotsvqa\n",
      "xwaxixb\n",
      "eqauodh\n",
      "bfeqjkj\n",
      "lxmcdon\n",
      "vmotyeg\n",
      "wtbmcoj\n",
      "cxqmchv\n"
     ]
    }
   ],
   "source": [
    "import weblinx as wl\n",
    "from pathlib import Path\n",
    "\n",
    "wl_dir = Path(\"./wl_data\")\n",
    "base_dir = wl_dir / \"demonstrations\"\n",
    "split_path = \"splits.json\"\n",
    "focused_url=    \"https://www.reddit.com/\"  #Select URL to filter demonstrations\n",
    "demos_with_url=[]\n",
    "\n",
    "\n",
    "# Filter demonstrations based on focused_url\n",
    "demos = [wl.Demonstration(name, base_dir=base_dir) for name in demo_names]\n",
    "for demo in demos:\n",
    "    replay = wl.Replay.from_demonstration(demo)\n",
    "    turns = replay.filter_by_intents(\"load\")\n",
    "    if len(turns) == 0:\n",
    "        continue\n",
    "    for turn in turns:\n",
    "        if focused_url in turn.url:\n",
    "            print(demo.name)\n",
    "            demos_with_url.append(demo.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts/reddit\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Could you please open reddit?\n",
      "ðŸ¤– Sure\n",
      "ðŸ¤– How can I help you?\n",
      "ðŸ§‘ Search World News & open /r/WorldNews Live Thread: Russian Invasion of Ukraine Day 384, Part 1 (Thread #525)\n",
      "ðŸ¤– Here it is!\n",
      "ðŸ§‘ Please comment on the post as \"End the War!\"\n",
      "ðŸ¤– Done. Your comment has been sent.\n",
      "ðŸ§‘ Please join the 'World News' community.\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Done\n",
      "ðŸ§‘ That's all for now.\n",
      "ðŸ¤– Alright.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Comment on the Reddit post \\'WorldNews Live Thread: Russian Invasion of Ukraine Day 384, Part 1 (Thread #525)\\' with the message \\'End the War!\\' and join the \\'World News\\' community.\"', 'steps': \"1. Open Reddit.\\n2. Search for 'World News' and find the live thread 'Russian Invasion of Ukraine Day 384, Part 1 (Thread #525).'\\n3. Comment on the post with the message 'End the War!'\\n4. Join the 'World News' community.\"}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Could you please open reddit?\n",
      "ðŸ¤– Sure\n",
      "ðŸ§‘ Create an Image post for the conservation of Siberian Tiger.\n",
      "ðŸ¤– Sure\n",
      "ðŸ§‘ Use the below title: \n",
      "\t\n",
      "\t\"ðŸ¯ Proud supporter of Siberian Tiger conservation! Together, we can protect these majestic creatures and their habitats.  ðŸŒ¿ðŸ… #SaveSiberianTigers #ConservationMatters\"\n",
      "ðŸ¤– Title added\n",
      "ðŸ§‘ Please add an Image of 'Siberian Tiger'.\n",
      "ðŸ¤– Image added\n",
      "ðŸ§‘ Save this post as a draft.\n",
      "ðŸ¤– Drafted\n",
      "ðŸ§‘ That's all for now.\n",
      "ðŸ¤– Alright.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Create a draft image post on Reddit for the conservation of Siberian Tigers.\"', 'steps': '1. Open Reddit.\\n2. Create a new image post.\\n3. Use the provided title: \"ðŸ¯ Proud supporter of Siberian Tiger conservation! Together, we can protect these majestic creatures and their habitats. ðŸŒ¿ðŸ… #SaveSiberianTigers #ConservationMatters\"\\n4. Add an image of a Siberian Tiger.\\n5. Save the post as a draft.'}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Could you please open Reddit?\n",
      "ðŸ¤– Sure\n",
      "ðŸ§‘ Please send a 'Holiday Greeting' message to u/Esytest.\n",
      "ðŸ¤– Can you help me with message?\n",
      "ðŸ§‘ Generate using ChatGPT.\n",
      "ðŸ¤– Alright.\n",
      "ðŸ¤– Can you specify the Holiday?\n",
      "ðŸ§‘ Diwali Holiday Greetings\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Here is the message generated using ChatGPT:\n",
      "\tWishing you a joyous and sparkling Diwali! May the festival of lights illuminate your path with happiness, prosperity, and endless opportunities. May the glow of diyas fill your home with warmth and love. May this auspicious occasion bring you closer to your loved ones and create cherished memories. Happy Diwali!\n",
      "ðŸ§‘ Perfect!\n",
      "ðŸ§‘ Please send it.\n",
      "ðŸ¤– Your message has been sent.\n",
      "ðŸ§‘ Thank you this will be all.\n",
      "ðŸ¤– Alright, you are welcome.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Send a \\'Holiday Greeting\\' message to u/Esytest on Reddit for the Diwali holiday.\"', 'steps': '1. Open Reddit.\\n2. Compose a holiday greeting message using ChatGPT.\\n3. Specify the holiday as Diwali.\\n4. Send the generated Diwali greeting message to u/Esytest.'}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Open reddit.\n",
      "ðŸ¤– Sure\n",
      "ðŸ§‘ Search for the Id: r/ShoppingDealsOnline\n",
      "ðŸ¤– Here it is.\n",
      "ðŸ§‘ Please follow them and upvote their product\n",
      "ðŸ¤– Done.\n",
      "ðŸ§‘ Also add a comment â€œMay I know the features of this product?â€\n",
      "ðŸ¤– Done\n",
      "ðŸ§‘ Save this post.\n",
      "ðŸ¤– Saved\n",
      "ðŸ§‘ That's all for now.\n",
      "ðŸ¤– Alright.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Follow and upvote a product on the subreddit r/ShoppingDealsOnline, add a comment asking for the product features, and save the post.\"', 'steps': '1. Open Reddit.\\n2. Search for the subreddit r/ShoppingDealsOnline.\\n3. Follow the subreddit and locate the product post.\\n4. Upvote the product post.\\n5. Add a comment asking for the product features.\\n6. Save the product post.'}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Open Reddit.\n",
      "ðŸ¤– Sure\n",
      "ðŸ¤– Here  it is.\n",
      "ðŸ§‘ I want to send an invite for a BGMI match, so you will have to create a post and add the paragraph below: \n",
      "\t\n",
      "\tðŸŽ® Calling all BGMI Teams! Join us for an Epic Erangel Showdown! ðŸ† \n",
      "\t\n",
      "\tðŸ“… Date: 02nd July 2023 \n",
      "\t\n",
      "\tâŒš Time: 4:00pm \n",
      "\t\n",
      "\tðŸ—ºï¸ Map: Erangel \n",
      "\t\n",
      "\t Hey BGMI Warriors! \n",
      "\t\n",
      "\tGet your squad ready because we're hosting a thrilling BGMI match on the iconic Erangel map! Prepare for intense battles, strategic gameplay, and unforgettable moments. It's time to prove your skills and claim victory! \n",
      "\t\n",
      "\tðŸŒŸ Event Details: \n",
      "\t\n",
      "\tðŸ“… Date: 02nd July 2023 \n",
      "\t\n",
      "\tâŒš Time: 4:00pm \n",
      "\t\n",
      "\tðŸ—ºï¸ Map: Erangel \n",
      "\t\n",
      "\tGather your team of 4 players and gear up for an adrenaline-fueled gaming session. This match will test your teamwork, coordination, and tactical prowess. It's an opportunity to show off your gaming prowess and have a blast!\n",
      "ðŸ§‘ To secure your team's spot, comment below with the following details: \n",
      "\t\n",
      "\tðŸ”¹ Team Name: \n",
      "\t\n",
      "\tðŸ”¹ Captain's Name: \n",
      "\t\n",
      "\tðŸ”¹ List of Team Members (including usernames): \n",
      "\t\n",
      "\tLimited spots are available, so don't wait! Join us on https://discord.gg/jc8YGMFn for further updates and coordination. \n",
      "\t\n",
      "\tSpread the word, tag your gaming buddies, and let's make this an unforgettable battleground experience!\n",
      "ðŸ§‘ Title:- Invite for a BGMI match\n",
      "ðŸ¤– Wait, I am adding the above post.\n",
      "ðŸ¤– Done\n",
      "ðŸ§‘ Add a BGMI image from google which would be suitable for the post.\n",
      "ðŸ¤– Image added.\n",
      "ðŸ§‘ Alright, please draft this post.\n",
      "ðŸ¤– Done\n",
      "ðŸ§‘ That wraps it up.\n",
      "ðŸ¤– If you have any more questions or need assistance in the future, don't hesitate to reach out. Have a great day!\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Create a Reddit post inviting BGMI teams for a match on Erangel with specific details and a call to action.\"', 'steps': '1. Open Reddit.\\n2. Create a post with the following paragraph:\\n\\tðŸŽ® Calling all BGMI Teams! Join us for an Epic Erangel Showdown! ðŸ†\\n\\tðŸ“… Date: 02nd July 2023\\n\\tâŒš Time: 4:00pm\\n\\tðŸ—ºï¸ Map: Erangel\\n\\tHey BGMI Warriors!\\n\\tGet your squad ready because we\\'re hosting a thrilling BGMI match on the iconic Erangel map! Prepare for intense battles, strategic gameplay, and unforgettable moments. It\\'s time to prove your skills and claim victory!\\n\\tðŸŒŸ Event Details:\\n\\tðŸ“… Date: 02nd July 2023\\n\\tâŒš Time: 4:00pm\\n\\tðŸ—ºï¸ Map: Erangel\\n\\tGather your team of 4 players and gear up for an adrenaline-fueled gaming session. This match will test your teamwork, coordination, and tactical prowess. It\\'s an opportunity to show off your gaming prowess and have a blast!\\n\\tTo secure your team\\'s spot, comment below with the following details:\\n\\tðŸ”¹ Team Name:\\n\\tðŸ”¹ Captain\\'s Name:\\n\\tðŸ”¹ List of Team Members (including usernames):\\n\\tLimited spots are available, so don\\'t wait! Join us on https://discord.gg/jc8YGMFn for further updates and coordination.\\n\\tSpread the word, tag your gaming buddies, and let\\'s make this an unforgettable battleground experience!\\n3. Title the post as \"Invite for a BGMI match.\"\\n4. Add a suitable BGMI image from Google to the post.\\n5. Draft the post.'}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Could you please open Reddit\n",
      "ðŸ¤– Sure\n",
      "ðŸ§‘ Go to u/Esytest profile & find if there is any post.\n",
      "ðŸ¤– Yes. There is a post titled \"Save Forest\".\n",
      "ðŸ§‘ Can you send a summary of the post?\n",
      "ðŸ¤– Yes, sure.\n",
      "ðŸ§‘ Please make use of ChatGPT.\n",
      "ðŸ§‘ Please continue with google\n",
      "ðŸ¤– Please share credential\n",
      "ðŸ§‘ webtasks.navigator@gmail.com\n",
      "\tKEG24qweUHij%^\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Here is the summary generated using ChatGPT:\n",
      "\tForests are crucial for sustaining biodiversity, acting as carbon sinks, regulating the water cycle, and preserving indigenous communities and cultural heritage. However, deforestation and unsustainable practices threaten their existence. To save forests, we must adopt sustainable practices, support conservation efforts, and raise awareness about their importance. By protecting forests, we invest in a greener and more sustainable planet for future generations.\n",
      "ðŸ§‘ Upvote the post & comment the below details: \n",
      "\t\n",
      "\t\n",
      "\t\"Thank you for highlighting the crucial role of forests in our ecosystem. It's imperative that we recognize the value of these biodiverse havens and take action to protect them. Sustainable practices and conservation efforts are vital to secure a greener future for all. Let's come together and make a difference for our planet's sake. #SaveOurForests #ConservationMatters ðŸŒ³ðŸ’š\"\n",
      "ðŸ¤– Comment added\n",
      "ðŸ§‘ Thanks a lot.\n",
      "ðŸ¤– Welcome\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Retrieve a summary of the post titled \\'Save Forest\\' from the Reddit profile u/Esytest, upvote the post, and comment with a specific message.\"', 'steps': '1. Open Reddit.\\n2. Navigate to the profile u/Esytest.\\n3. Find and retrieve the post titled \"Save Forest.\"\\n4. Generate a summary of the post using ChatGPT.\\n5. Upvote the post.\\n6. Comment on the post with the specified message.'}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hii\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Could you please open Reddit?\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Here it is!\n",
      "ðŸ§‘ Click on create post and add the title â€œComputer Scienceâ€\n",
      "ðŸ¤– Title added\n",
      "ðŸ¤– Can you help me with the description?\n",
      "ðŸ§‘ Computer Science is the study of computers and computational systems. Unlike electrical and computer engineers, computer scientists deal mostly with software and software systems; this includes their theory, design, development, and application.\n",
      "ðŸ¤– Done\n",
      "ðŸ§‘ Please post it.\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Post option is not available\n",
      "ðŸ¤– Can I save it as draft?\n",
      "ðŸ§‘ Alright. Save it as 'Drafts.'\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Done\n",
      "ðŸ§‘ We can stop here.\n",
      "ðŸ¤– Sure thing, if you have any more questions in the future, don't hesitate to ask.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Create a Reddit post with the title \\'Computer Science\\' and a description about the study of computers and computational systems, then save it as a draft.\"', 'steps': \"1. Open Reddit.\\n2. Click on 'Create Post.'\\n3. Add the title 'Computer Science.'\\n4. Provide the description about the study of computers and computational systems.\\n5. Attempt to post the content.\\n6. If posting is not available, save the post as a draft.\"}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Open website reddit.\n",
      "ðŸ¤– Sure\n",
      "ðŸ¤– What do you want me to do?\n",
      "ðŸ§‘ Search post â€œSave Forestâ€ by u/Esytest.\n",
      "ðŸ¤– Here it is!\n",
      "ðŸ§‘ Add a comment as below: \n",
      "\tWow, what an insightful and thought-provoking blog post! The importance of saving our forests cannot be overstated. The way you've highlighted the various aspects of forest conservation, from biodiversity to climate regulation and cultural heritage, truly emphasizes the interconnectedness of these ecosystems with our lives. It's crucial that we take action now to protect and preserve our forests for future generations. I'm inspired by the sustainable practices and conservation efforts you mentioned. Together, we can make a positive impact and ensure a greener and more sustainable planet. Thank you for shedding light on this important topic!\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Done. You want to add any tags?\n",
      "ðŸ§‘ Yes, add â€œ#SaveForests #PreserveNature #BiodiversityMatters #ClimateAction #WaterConservation #CulturalHeritage #Sustainability #CommunityEngagement #TogetherForChange #GreenPlanet â€œ\n",
      "ðŸ¤– Done . Anything else do you want me to do?\n",
      "ðŸ§‘ No, That's all for now.\n",
      "ðŸ¤– Alright.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Add a comment and tags to the post \\'Save Forest\\' by u/Esytest on Reddit.\"', 'steps': '1. Open the Reddit website.\\n2. Search for the post \"Save Forest\" by u/Esytest.\\n3. Add the following comment:\\n   \"Wow, what an insightful and thought-provoking blog post! The importance of saving our forests cannot be overstated. The way you\\'ve highlighted the various aspects of forest conservation, from biodiversity to climate regulation and cultural heritage, truly emphasizes the interconnectedness of these ecosystems with our lives. It\\'s crucial that we take action now to protect and preserve our forests for future generations. I\\'m inspired by the sustainable practices and conservation efforts you mentioned. Together, we can make a positive impact and ensure a greener and more sustainable planet. Thank you for shedding light on this important topic!\"\\n4. Add the following tags:\\n   \"#SaveForests #PreserveNature #BiodiversityMatters #ClimateAction #WaterConservation #CulturalHeritage #Sustainability #CommunityEngagement #TogetherForChange #GreenPlanet\"'}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Could you please open reddit?\n",
      "ðŸ¤– Sure\n",
      "ðŸ§‘ Please create a post on â€œTypes of tortoisesâ€ using ChatGPT\n",
      "ðŸ¤– Alright!\n",
      "ðŸ¤– Here is what I found:\n",
      "\tHere are some types of tortoises:\n",
      "\t\n",
      "\t-Galapagos Tortoise\n",
      "\t-African Spurred Tortoise\n",
      "\t-Aldabra Tortoise\n",
      "\t-Leopard Tortoise\n",
      "\t-Red-footed Tortoise\n",
      "\t-Russian Tortoise\n",
      "\t-Indian Star Tortoise\n",
      "\t-Hermann's Tortoise\n",
      "\tEach of these tortoises has its own distinct features, habitat, and characteristics.\n",
      "ðŸ§‘ Looks good, please add the above content to the post.\n",
      "ðŸ¤– Sure\n",
      "ðŸ¤– The content is added.\n",
      "ðŸ¤– Do you want to add tags for this post?\n",
      "ðŸ§‘ Yes, add the following tags: \n",
      "\t#TortoiseSpecies #TortoiseConservation #ReptilesOfTheWorld #TortoiseDiversity #TortoiseHabitats #AncientReptiles #TortoiseAdaptations #ProtectingTortoises #WildlifeConservation #TortoiseAwareness #TortoiseLovers #NatureExploration #EcologicalBalance #PreservingBiodiversity #TortoiseEnthusiasts\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Tags added\n",
      "ðŸ§‘ Please add an image of a tortoise using google.\n",
      "ðŸ¤– Alright!\n",
      "ðŸ¤– Image added\n",
      "ðŸ§‘ Now save it as Draft\n",
      "ðŸ¤– Done\n",
      "ðŸ§‘ That's all for now.\n",
      "ðŸ¤– Alright.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Create a Reddit post about \\'Types of tortoises\\' using ChatGPT, add content about various tortoise species, include specified tags, add an image of a tortoise from Google, and save the post as a draft.\"', 'steps': '1. Open Reddit.\\n2. Create a post about \"Types of tortoises\" using ChatGPT.\\n3. Add the provided content about different tortoise species.\\n4. Include the specified tags: \\n   #TortoiseSpecies #TortoiseConservation #ReptilesOfTheWorld #TortoiseDiversity #TortoiseHabitats #AncientReptiles #TortoiseAdaptations #ProtectingTortoises #WildlifeConservation #TortoiseAwareness #TortoiseLovers #NatureExploration #EcologicalBalance #PreservingBiodiversity #TortoiseEnthusiasts\\n5. Add an image of a tortoise from Google.\\n6. Save the post as a draft.'}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Could you please open reddit?\n",
      "ðŸ¤– Sure\n",
      "ðŸ§‘ Please create a post for me.\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Please help me with the contents for the post.\n",
      "ðŸ§‘ Title will be 'I tried a new recipe, and it turned out amazing!'.\n",
      "ðŸ¤– Title added.\n",
      "ðŸ¤– What will the body be?\n",
      "ðŸ§‘ Calling all foodies! Last night, I decided to try my hand at cooking Omelette for the first time, and it was a huge success. The flavors were spot on, and it was surprisingly easy to make. If you're looking to impress your taste buds, give this recipe a try. I've included the link in the comments. Bon appÃ©tit!\n",
      "ðŸ¤– Do you want to format the text?\n",
      "ðŸ§‘ No. Just draft it.\n",
      "ðŸ¤– Sure\n",
      "ðŸ¤– Drafted.\n",
      "ðŸ§‘ That's all for now.\n",
      "ðŸ¤– Alright.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Create a Reddit post with the title \\'I tried a new recipe, and it turned out amazing!\\' and the specified content.\"', 'steps': '1. Open Reddit.\\n2. Create a new post.\\n3. Set the post title as \\'I tried a new recipe, and it turned out amazing!\\'.\\n4. Add the following content to the post:\\n   \"Calling all foodies! Last night, I decided to try my hand at cooking Omelette for the first time, and it was a huge success. The flavors were spot on, and it was surprisingly easy to make. If you\\'re looking to impress your taste buds, give this recipe a try. I\\'ve included the link in the comments. Bon appÃ©tit!\"\\n5. Draft the post without formatting.'}\n",
      "--------------------------------------------------------\n",
      "ðŸ§‘ Hi\n",
      "ðŸ¤– Hello\n",
      "ðŸ§‘ Could you please open reddit website?\n",
      "ðŸ¤– Okay\n",
      "ðŸ§‘ Create a community named as â€œRecipeLoversâ€\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Please choose the community type from the options below:\n",
      "\t- Public\n",
      "\t- Restricted\n",
      "\t- Private\n",
      "ðŸ§‘ Choose public and select adult content.\n",
      "ðŸ¤– Done. Do you want me to create a post?\n",
      "ðŸ§‘ Yes, create a post and add the below recipe:\n",
      "\tTitle: A classic recipe for Tiramisu \n",
      "\t\n",
      "\tIngredients: \n",
      "\t\n",
      "\t     6 egg yolks \n",
      "\t\n",
      "\t    3/4 cup granulated sugar \n",
      "\t\n",
      "\t    2/3 cup milk \n",
      "\t\n",
      "\t    1 1/4 cups heavy cream \n",
      "\t\n",
      "\t    1/2 teaspoon vanilla extract \n",
      "\t\n",
      "\t    8 ounces mascarpone cheese \n",
      "\t\n",
      "\t    1 cup strong brewed coffee, cooled \n",
      "\t\n",
      "\t    2 tablespoons rum or coffee liqueur (optional) \n",
      "\t\n",
      "\t    24 ladyfinger cookies \n",
      "\t\n",
      "\t    Unsweetened cocoa powder, for dusting\n",
      "ðŸ¤– Can I add bullet points in ingredients?\n",
      "ðŸ§‘ Yes. Please.\n",
      "ðŸ¤– Done\n",
      "ðŸ¤– Anything else?\n",
      "ðŸ§‘ Please put on instructions in the description after bullet points.\n",
      "ðŸ¤– Okay\n",
      "ðŸ¤– Please send\n",
      "ðŸ§‘ Instructions: \n",
      "\t\n",
      "\tIn a large mixing bowl, whisk together the egg yolks and sugar until well combined and creamy. \n",
      "\t\n",
      "\tIn a saucepan, heat the milk over medium heat until it just begins to simmer. Remove from heat. \n",
      "\t\n",
      "\tSlowly pour the hot milk into the egg yolk mixture while whisking continuously to temper the eggs. \n",
      "\t\n",
      "\tTransfer the mixture back to the saucepan and cook over low heat, stirring constantly, until the mixture thickens and coats the back of a spoon. Remove from heat and let it cool to room temperature.\n",
      "ðŸ§‘ In a separate bowl, beat the heavy cream and vanilla extract until stiff peaks form. \n",
      "\t\n",
      "\tGently fold the mascarpone cheese into the cooled egg yolk mixture until well combined. \n",
      "\t\n",
      "\tGradually fold in the whipped cream until the mixture is smooth and creamy. \n",
      "\t\n",
      "\tIn a shallow dish, combine the brewed coffee and rum or coffee liqueur if using. \n",
      "\t\n",
      "\tDip each ladyfinger cookie into the coffee mixture briefly, making sure not to soak them completely. \n",
      "\t\n",
      "\tArrange a layer of soaked ladyfingers in the bottom of a rectangular or square dish. \n",
      "\t\n",
      "\tSpread half of the mascarpone cream mixture evenly over the ladyfingers. \n",
      "\t\n",
      "\tRepeat the layers with another layer of soaked ladyfingers and the remaining mascarpone cream. \n",
      "\t\n",
      "\tCover the dish with plastic wrap and refrigerate for at least 4 hours or overnight to allow the flavors to meld and the tiramisu to set. \n",
      "\t\n",
      "\tJust before serving, dust the top of the tiramisu with a generous amount of cocoa powder. \n",
      "\t\n",
      "\tSlice and serve chilled. Enjoy!\n",
      "ðŸ¤– Done\n",
      "ðŸ¤– Would you like to add any tags for the above post?\n",
      "ðŸ§‘ Add the below tags: \n",
      "\t#TiramisuRecipe #ClassicDesserts #ItalianCuisine #EasyDesserts #CoffeeFlavors #IndulgentTreats #MascarponeCheese\n",
      "ðŸ¤– Done\n",
      "ðŸ¤– Can I click on \"Post\" option?\n",
      "ðŸ§‘ Yes. Please.\n",
      "ðŸ¤– Done\n",
      "ðŸ§‘ We are done here.\n",
      "\n",
      "prompts/reddit\n",
      "{'goal': '\"Create a community named \\'RecipeLovers\\' on Reddit, make a public community with adult content, and create a post with a classic Tiramisu recipe including ingredients and instructions, along with specific tags.\"', 'steps': '1. Open the Reddit website.\\n2. Create a community named \"RecipeLovers.\"\\n3. Choose the community type as public and select adult content.\\n4. Create a post with the following details:\\n   - Title: A classic recipe for Tiramisu\\n   - Ingredients with bullet points\\n   - Instructions in the description after bullet points\\n5. Add the following tags to the post: #TiramisuRecipe #ClassicDesserts #ItalianCuisine #EasyDesserts #CoffeeFlavors #IndulgentTreats #MascarponeCheese\\n6. Click on the \"Post\" option to publish the post.'}\n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extract step and goals based on demonstration\n",
    "prompt_dir= f'prompts/{focused_url.split('.')[1]}'\n",
    "demos = [wl.Demonstration(name, base_dir=base_dir) for name in demos_with_url]\n",
    "for i,demo in enumerate(demos):\n",
    "    replay = wl.Replay.from_demonstration(demo)\n",
    "    turns = replay.filter_by_type(\"chat\")\n",
    "    if len(turns) == 0:\n",
    "        continue\n",
    "    stringBuilder= \"\"\n",
    "    for turn in turns:    \n",
    "        stringBuilder+=(format_chat_message(turn))+'\\n'\n",
    "    print(stringBuilder)\n",
    "    steps_goal= adapt_text(stringBuilder,3)\n",
    "    with open(f'{prompt_dir}/question{i}.txt', 'w+') as f:\n",
    "        f.write(steps_goal['goal'])\n",
    "    with open(f'{prompt_dir}/answer{i}.txt', 'w+') as f:\n",
    "        f.write(steps_goal['steps'])\n",
    "    print('--------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference (step prediction based on goal)\n",
    "Here we are testing the few-shot capability of creating good plans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_steps(text, n_retry=3):\n",
    "  client = OpenAI(api_key=os.environ.get(\"OPENAI\"))\n",
    "  tries= 0\n",
    "  while tries < n_retry:\n",
    "    try:\n",
    "      prompt_dir= 'prompts/reddit'\n",
    "      mainPrompt= open(f'{prompt_dir}/mainPrompt.txt').read()\n",
    "      mainResponse= open(f'{prompt_dir}/mainResponse.txt').read()\n",
    "      messages = create_prompt(mainPrompt,mainResponse,prompt_dir)\n",
    "      messages.append({\"role\": \"user\", \"content\": f\"Create steps for the goal:{text}\"})\n",
    "      response = client.chat.completions.create(\n",
    "                    model=\"gpt-3.5-turbo-0125\",\n",
    "                    temperature=0.2,\n",
    "                    messages = messages\n",
    "                )\n",
    "      reply_content = response.choices[0].message.content\n",
    "\n",
    "      return reply_content\n",
    "    except ParseError as parsing_error:\n",
    "            tries += 1\n",
    "\n",
    "    raise ParseError(f\"Could not parse a valid value after {n_retry} retries.\")\n",
    "        \n",
    "  return reply_content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Open Reddit and navigate to the Showerthoughts forum.\n",
      "2. Identify the latest post made on the forum.\n",
      "3. Click on the post to view the user who made the post.\n",
      "4. Check the comments section of the post.\n",
      "5. For each comment, calculate the number of downvotes and upvotes.\n",
      "6. Count the comments where the downvotes are greater than the upvotes.\n",
      "7. Provide the user with the count of comments that meet this criteria.\n"
     ]
    }
   ],
   "source": [
    "print (get_steps(\"Tell me the count of comments that have received more downvotes than upvotes for the user who made the latest post on the Showerthoughts forum.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Open Reddit.\n",
      "2. Navigate to the \"books\" subreddit.\n",
      "3. Locate the top 10 posts.\n",
      "4. Identify the posts that recommend a single book.\n",
      "5. Provide the URLs of those specific posts.\n"
     ]
    }
   ],
   "source": [
    "print(get_steps(\"Among the top 10 post in \\\"books\\\" forum, show me the post URLs that recommand a single book\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Open Reddit.\n",
      "2. Go to your profile.\n",
      "3. Find the section for editing your bio.\n",
      "4. Change your bio text to \"I am a robot.\"\n",
      "5. Save the changes.\n"
     ]
    }
   ],
   "source": [
    "print(get_steps('Change my reddit bio to \"I am a robot\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Open Reddit.\n",
      "2. Navigate to the \"Books\" subreddit.\n",
      "3. Sort the posts by \"Newest\" to find the latest post.\n",
      "4. Upvote the newest post in the \"Books\" subreddit.\n"
     ]
    }
   ],
   "source": [
    "print(get_steps(\"Upvote the newest post in books subreddit\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
